{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import optuna\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the objective function for Optuna optimization\n",
    "def objective(trial):\n",
    "    # Load dataset (replace this with your actual dataset loading)\n",
    "    X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "\n",
    "    # Split dataset into training and validation sets\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define hyperparameters to tune\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "\n",
    "    # Define empty list to store cross-validation scores\n",
    "    scores = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_index, valid_index in skf.split(X, y):\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "\n",
    "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "        dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "        # Train LightGBM model\n",
    "        model = lgb.train(params, dtrain, valid_sets=[dvalid], early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "        # Predict validation set and calculate binary log loss\n",
    "        y_pred = model.predict(X_valid)\n",
    "        score = sklearn.metrics.log_loss(y_valid, y_pred)\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    # Return average binary log loss over all folds\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Create study object and optimize hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print best trial parameters and value\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('  Value: {:.4f}'.format(trial.value))\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Requirement Document: Python Automation Script for File Processing\n",
    "\n",
    "1. Introduction:\n",
    "The purpose of this document is to outline the business requirements for a Python automation script designed to streamline file processing tasks. The script is intended to automate the processing of six files, organized into three distinct processes, with the ability to perform checks on each process and draft corresponding emails.\n",
    "\n",
    "2. Background:\n",
    "In our organization, we frequently encounter manual file processing tasks that are time-consuming and prone to errors. These tasks involve handling six files, grouped into three processes, where each process consists of two files. To address this challenge, we have developed a Python automation script that will automate these file processing tasks, improve efficiency, and reduce errors.\n",
    "\n",
    "3. Objectives:\n",
    "The primary objectives of the Python automation script are as follows:\n",
    "\n",
    "Automate the processing of six files grouped into three processes.\n",
    "Perform checks on each process to ensure data integrity and completeness.\n",
    "Draft and send emails based on the outcome of the processing, providing necessary notifications to stakeholders.\n",
    "4. Scope:\n",
    "The scope of the automation script includes the following:\n",
    "\n",
    "Processing of six input files, organized into three processes: Process A, Process B, and Process C.\n",
    "Implementation of checks on each process to validate data integrity and completeness.\n",
    "Generation of email notifications summarizing the processing results and any detected issues.\n",
    "5. Features:\n",
    "The key features of the automation script include:\n",
    "\n",
    "File Processing: The script will read, process, and analyze the contents of six input files.\n",
    "Checks and Validation: Each process will undergo checks to ensure that the data meets predefined criteria and standards.\n",
    "Email Notification: The script will draft email notifications summarizing the processing results, including any issues detected during validation.\n",
    "6. Process Overview:\n",
    "The three processes to be automated by the script are described below:\n",
    "\n",
    "Process A: Involves the processing of two input files related to a specific task or operation.\n",
    "Process B: Includes the processing of two additional input files, distinct from those in Process A.\n",
    "Process C: The final process entails the processing of the remaining two input files, separate from those in Processes A and B.\n",
    "7. Deliverables:\n",
    "The deliverables of the automation script include:\n",
    "\n",
    "Processed Files: Six output files containing the results of the processing for each process.\n",
    "Email Notifications: Automatically generated email notifications providing a summary of the processing results and any detected issues.\n",
    "8. Assumptions:\n",
    "The following assumptions are made regarding the automation script:\n",
    "\n",
    "The input files are provided in a predefined format and location accessible to the script.\n",
    "The script will run on a scheduled basis to ensure timely processing of the files.\n",
    "Email configuration details, including SMTP server information, are preconfigured for sending notifications.\n",
    "9. Acceptance Criteria:\n",
    "The automation script will be considered successful if it meets the following acceptance criteria:\n",
    "\n",
    "Successfully processes all six input files without errors.\n",
    "Performs checks on each process and flags any issues or discrepancies detected.\n",
    "Generates accurate email notifications summarizing the processing results and any identified issues.\n",
    "10. Conclusion:\n",
    "The Python automation script described in this document will significantly enhance our file processing capabilities, leading to improved efficiency, reduced errors, and enhanced stakeholder communication. By automating repetitive tasks and implementing checks for data validation, the script will contribute to greater operational efficiency and productivity within our organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is first segregated into two categories: Single and Net Cash Flows\n",
    "Net Cash Flows have payment types as “OTC NET Cash Flow” regardless of the different product types in the system. Hence, for this reason, the gross level payment types are consolidated using a product-payment type consolidation file. \n",
    "The Breaks/Fails model is trained on a dataset of two years’ data which spans across 105 product types and 110 payment types. When a new flow is passed into the model, the model predicts the outcome based on the historical data. The model considers the product type, payment type, counterparty name, cash flow amount, execution status, currency, and NTRM Type. Based on these factors, the model makes a prediction as STP only when it has 95% probability of having no future fail/break based on the historical data.\n",
    "The model uses multiple decision trees to make a prediction. The training data was subsampled into smaller parts to train multiple decision trees and we use a voting classifier to get the average of all the decision tree classifications which generates the probability score. \n",
    "\n",
    "The historical data of breaks/fails that was used to prepare the first model has a gap. The breaks and fails that had come in the past two years data was after the pre-teams efforts which matched a lot of prior non-stp flows because of which they might not have had a  break/fail. This is why we have trained a new model with the target variable being pre-efforts. It has data of the flows which the pre-settlements team has worked on for the last four months and the model aims to identify whether a trade would require pre-team efforts. Hence even if breaks/fails model predicts a flow as STP but the pre-efforts model predicts the same as NSTP then the trade would be marked as NSTP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import wordninja\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('comments.csv')  # Replace with your actual file path\n",
    "\n",
    "# Define a function to clean, segment, and preprocess the text\n",
    "def clean_and_preprocess(text):\n",
    "    # Remove numerical characters\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Add spaces between words\n",
    "    words = wordninja.split(text)\n",
    "    text = ' '.join(words)\n",
    "    # Remove stop words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the function to the 'Comments' column\n",
    "data['Cleaned_Comments'] = data['Comments'].apply(clean_and_preprocess)\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data['Cleaned_Comments'])\n",
    "\n",
    "# Apply K-means clustering\n",
    "num_clusters = 5  # Choose the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "data['Cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Analyze clusters\n",
    "for i in range(num_clusters):\n",
    "    cluster_comments = data[data['Cluster'] == i]['Comments']\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(cluster_comments.head())\n",
    "    print(\"\\n\")\n",
    "\n",
    "# To find the most common words in each cluster\n",
    "from collections import Counter\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_words = ' '.join(data[data['Cluster'] == i]['Cleaned_Comments']).split()\n",
    "    common_words = Counter(cluster_words).most_common(10)\n",
    "    print(f\"Cluster {i} common words: {common_words}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Save the cleaned data to a new CSV file (optional)\n",
    "data.to_csv('cleaned_comments_with_clusters.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordsegment import load, segment\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load wordsegment data\n",
    "load()\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('comments.csv')  # Replace with your actual file path\n",
    "\n",
    "# Define a function to clean and preprocess the text\n",
    "def clean_and_preprocess(text):\n",
    "    # Remove text before the first hyphen\n",
    "    text = re.sub(r'^.*?-', '', text)\n",
    "    # Remove numerical characters\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Add spaces between words\n",
    "    words = segment(text)\n",
    "    text = ' '.join(words)\n",
    "    # Remove stop words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Use multiprocessing to speed up the cleaning process\n",
    "def apply_preprocessing(texts):\n",
    "    with Pool() as pool:\n",
    "        return pool.map(clean_and_preprocess, texts)\n",
    "\n",
    "# Apply the preprocessing function to the 'Comments' column\n",
    "data['Cleaned_Comments'] = apply_preprocessing(data['Comments'].tolist())\n",
    "\n",
    "# Feature extraction using TF-IDF with n-grams (bigrams and trigrams)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_df=0.85, min_df=2)\n",
    "X = vectorizer.fit_transform(data['Cleaned_Comments'])\n",
    "\n",
    "# Apply K-means clustering\n",
    "num_clusters = 5  # Choose the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "data['Cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Function to extract top n-grams from a cluster\n",
    "def get_top_ngrams(cluster_data, n=10):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "    X = vectorizer.fit_transform(cluster_data)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    sums = X.sum(axis=0)\n",
    "    data = []\n",
    "    for col, term in enumerate(terms):\n",
    "        data.append((term, sums[0, col]))\n",
    "    ranking = pd.DataFrame(data, columns=['term', 'rank'])\n",
    "    words = ranking.sort_values('rank', ascending=False)\n",
    "    return words.head(n)\n",
    "\n",
    "# Analyze clusters\n",
    "for i in range(num_clusters):\n",
    "    cluster_comments = data[data['Cluster'] == i]['Cleaned_Comments']\n",
    "    print(f\"Cluster {i}:\")\n",
    "    top_ngrams = get_top_ngrams(cluster_comments)\n",
    "    print(top_ngrams)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Save the cleaned data to a new CSV file (optional)\n",
    "data.to_csv('cleaned_comments_with_clusters.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
