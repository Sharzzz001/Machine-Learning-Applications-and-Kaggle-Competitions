{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import optuna\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the objective function for Optuna optimization\n",
    "def objective(trial):\n",
    "    # Load dataset (replace this with your actual dataset loading)\n",
    "    X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "\n",
    "    # Split dataset into training and validation sets\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define hyperparameters to tune\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "\n",
    "    # Define empty list to store cross-validation scores\n",
    "    scores = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_index, valid_index in skf.split(X, y):\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "\n",
    "        dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "        dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "        # Train LightGBM model\n",
    "        model = lgb.train(params, dtrain, valid_sets=[dvalid], early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "        # Predict validation set and calculate binary log loss\n",
    "        y_pred = model.predict(X_valid)\n",
    "        score = sklearn.metrics.log_loss(y_valid, y_pred)\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    # Return average binary log loss over all folds\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Create study object and optimize hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print best trial parameters and value\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('  Value: {:.4f}'.format(trial.value))\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Requirement Document: Python Automation Script for File Processing\n",
    "\n",
    "1. Introduction:\n",
    "The purpose of this document is to outline the business requirements for a Python automation script designed to streamline file processing tasks. The script is intended to automate the processing of six files, organized into three distinct processes, with the ability to perform checks on each process and draft corresponding emails.\n",
    "\n",
    "2. Background:\n",
    "In our organization, we frequently encounter manual file processing tasks that are time-consuming and prone to errors. These tasks involve handling six files, grouped into three processes, where each process consists of two files. To address this challenge, we have developed a Python automation script that will automate these file processing tasks, improve efficiency, and reduce errors.\n",
    "\n",
    "3. Objectives:\n",
    "The primary objectives of the Python automation script are as follows:\n",
    "\n",
    "Automate the processing of six files grouped into three processes.\n",
    "Perform checks on each process to ensure data integrity and completeness.\n",
    "Draft and send emails based on the outcome of the processing, providing necessary notifications to stakeholders.\n",
    "4. Scope:\n",
    "The scope of the automation script includes the following:\n",
    "\n",
    "Processing of six input files, organized into three processes: Process A, Process B, and Process C.\n",
    "Implementation of checks on each process to validate data integrity and completeness.\n",
    "Generation of email notifications summarizing the processing results and any detected issues.\n",
    "5. Features:\n",
    "The key features of the automation script include:\n",
    "\n",
    "File Processing: The script will read, process, and analyze the contents of six input files.\n",
    "Checks and Validation: Each process will undergo checks to ensure that the data meets predefined criteria and standards.\n",
    "Email Notification: The script will draft email notifications summarizing the processing results, including any issues detected during validation.\n",
    "6. Process Overview:\n",
    "The three processes to be automated by the script are described below:\n",
    "\n",
    "Process A: Involves the processing of two input files related to a specific task or operation.\n",
    "Process B: Includes the processing of two additional input files, distinct from those in Process A.\n",
    "Process C: The final process entails the processing of the remaining two input files, separate from those in Processes A and B.\n",
    "7. Deliverables:\n",
    "The deliverables of the automation script include:\n",
    "\n",
    "Processed Files: Six output files containing the results of the processing for each process.\n",
    "Email Notifications: Automatically generated email notifications providing a summary of the processing results and any detected issues.\n",
    "8. Assumptions:\n",
    "The following assumptions are made regarding the automation script:\n",
    "\n",
    "The input files are provided in a predefined format and location accessible to the script.\n",
    "The script will run on a scheduled basis to ensure timely processing of the files.\n",
    "Email configuration details, including SMTP server information, are preconfigured for sending notifications.\n",
    "9. Acceptance Criteria:\n",
    "The automation script will be considered successful if it meets the following acceptance criteria:\n",
    "\n",
    "Successfully processes all six input files without errors.\n",
    "Performs checks on each process and flags any issues or discrepancies detected.\n",
    "Generates accurate email notifications summarizing the processing results and any identified issues.\n",
    "10. Conclusion:\n",
    "The Python automation script described in this document will significantly enhance our file processing capabilities, leading to improved efficiency, reduced errors, and enhanced stakeholder communication. By automating repetitive tasks and implementing checks for data validation, the script will contribute to greater operational efficiency and productivity within our organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is first segregated into two categories: Single and Net Cash Flows\n",
    "Net Cash Flows have payment types as “OTC NET Cash Flow” regardless of the different product types in the system. Hence, for this reason, the gross level payment types are consolidated using a product-payment type consolidation file. \n",
    "The Breaks/Fails model is trained on a dataset of two years’ data which spans across 105 product types and 110 payment types. When a new flow is passed into the model, the model predicts the outcome based on the historical data. The model considers the product type, payment type, counterparty name, cash flow amount, execution status, currency, and NTRM Type. Based on these factors, the model makes a prediction as STP only when it has 95% probability of having no future fail/break based on the historical data.\n",
    "The model uses multiple decision trees to make a prediction. The training data was subsampled into smaller parts to train multiple decision trees and we use a voting classifier to get the average of all the decision tree classifications which generates the probability score. \n",
    "\n",
    "The historical data of breaks/fails that was used to prepare the first model has a gap. The breaks and fails that had come in the past two years data was after the pre-teams efforts which matched a lot of prior non-stp flows because of which they might not have had a  break/fail. This is why we have trained a new model with the target variable being pre-efforts. It has data of the flows which the pre-settlements team has worked on for the last four months and the model aims to identify whether a trade would require pre-team efforts. Hence even if breaks/fails model predicts a flow as STP but the pre-efforts model predicts the same as NSTP then the trade would be marked as NSTP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import wordninja\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('comments.csv')  # Replace with your actual file path\n",
    "\n",
    "# Define a function to clean, segment, and preprocess the text\n",
    "def clean_and_preprocess(text):\n",
    "    # Remove numerical characters\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Add spaces between words\n",
    "    words = wordninja.split(text)\n",
    "    text = ' '.join(words)\n",
    "    # Remove stop words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the function to the 'Comments' column\n",
    "data['Cleaned_Comments'] = data['Comments'].apply(clean_and_preprocess)\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data['Cleaned_Comments'])\n",
    "\n",
    "# Apply K-means clustering\n",
    "num_clusters = 5  # Choose the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "data['Cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Analyze clusters\n",
    "for i in range(num_clusters):\n",
    "    cluster_comments = data[data['Cluster'] == i]['Comments']\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(cluster_comments.head())\n",
    "    print(\"\\n\")\n",
    "\n",
    "# To find the most common words in each cluster\n",
    "from collections import Counter\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_words = ' '.join(data[data['Cluster'] == i]['Cleaned_Comments']).split()\n",
    "    common_words = Counter(cluster_words).most_common(10)\n",
    "    print(f\"Cluster {i} common words: {common_words}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Save the cleaned data to a new CSV file (optional)\n",
    "data.to_csv('cleaned_comments_with_clusters.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordsegment import load, segment\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load wordsegment data\n",
    "load()\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('comments.csv')  # Replace with your actual file path\n",
    "\n",
    "# Define a function to clean and preprocess the text\n",
    "def clean_and_preprocess(text):\n",
    "    # Remove text before the first hyphen\n",
    "    text = re.sub(r'^.*?-', '', text)\n",
    "    # Remove numerical characters\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Add spaces between words\n",
    "    words = segment(text)\n",
    "    text = ' '.join(words)\n",
    "    # Remove stop words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Use multiprocessing to speed up the cleaning process\n",
    "def apply_preprocessing(texts):\n",
    "    with Pool() as pool:\n",
    "        return pool.map(clean_and_preprocess, texts)\n",
    "\n",
    "# Apply the preprocessing function to the 'Comments' column\n",
    "data['Cleaned_Comments'] = apply_preprocessing(data['Comments'].tolist())\n",
    "\n",
    "# Feature extraction using TF-IDF with n-grams (bigrams and trigrams)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_df=0.85, min_df=2)\n",
    "X = vectorizer.fit_transform(data['Cleaned_Comments'])\n",
    "\n",
    "# Apply K-means clustering\n",
    "num_clusters = 5  # Choose the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "data['Cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Function to extract top n-grams from a cluster\n",
    "def get_top_ngrams(cluster_data, n=10):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "    X = vectorizer.fit_transform(cluster_data)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    sums = X.sum(axis=0)\n",
    "    data = []\n",
    "    for col, term in enumerate(terms):\n",
    "        data.append((term, sums[0, col]))\n",
    "    ranking = pd.DataFrame(data, columns=['term', 'rank'])\n",
    "    words = ranking.sort_values('rank', ascending=False)\n",
    "    return words.head(n)\n",
    "\n",
    "# Analyze clusters\n",
    "for i in range(num_clusters):\n",
    "    cluster_comments = data[data['Cluster'] == i]['Cleaned_Comments']\n",
    "    print(f\"Cluster {i}:\")\n",
    "    top_ngrams = get_top_ngrams(cluster_comments)\n",
    "    print(top_ngrams)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Save the cleaned data to a new CSV file (optional)\n",
    "data.to_csv('cleaned_comments_with_clusters.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordsegment import load, segment\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load wordsegment data\n",
    "load()\n",
    "\n",
    "# Load spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('comments.csv')  # Replace with your actual file path\n",
    "\n",
    "# Define a function to clean and preprocess the text\n",
    "def clean_and_preprocess(text):\n",
    "    # Remove text before the first hyphen\n",
    "    text = re.sub(r'^.*?-', '', text)\n",
    "    # Remove numerical characters\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Add spaces between words\n",
    "    words = segment(text)\n",
    "    text = ' '.join(words)\n",
    "    # Remove stop words and lemmatize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    lemmatized_tokens = [token.lemma_ for token in nlp(' '.join(tokens))]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Use multiprocessing to speed up the cleaning process\n",
    "def apply_preprocessing(texts):\n",
    "    with Pool() as pool:\n",
    "        return pool.map(clean_and_preprocess, texts)\n",
    "\n",
    "# Apply the preprocessing function to the 'Comments' column\n",
    "data['Cleaned_Comments'] = apply_preprocessing(data['Comments'].tolist())\n",
    "\n",
    "# Convert the cleaned comments to a document-term matrix\n",
    "vectorizer = CountVectorizer(max_df=0.85, min_df=2, ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(data['Cleaned_Comments'])\n",
    "\n",
    "# Apply LDA\n",
    "num_topics = 5  # Choose the number of topics\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Function to display topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "# Display the top words in each topic\n",
    "no_top_words = 10\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, tf_feature_names, no_top_words)\n",
    "\n",
    "# Save the cleaned data to a new CSV file (optional)\n",
    "data.to_csv('cleaned_comments_with_topics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pitch:\n",
    "\"Today, we will delve into the power of SHAP values in understanding the factors driving our model's predictions. SHAP values provide a clear breakdown of how each predictor influences the outcome, offering invaluable transparency and interpretability. Our goal is to show how these insights can help us make informed decisions to drive success and avoid potential pitfalls.\"\n",
    "\n",
    "Slide 2: Understanding SHAP Values\n",
    "Title: Decoding SHAP Values\n",
    "\n",
    "Pitch:\n",
    "\"SHAP values allow us to decompose a prediction into contributions from each feature. This decomposition helps us understand which factors are pushing the prediction towards success or failure. By visualizing these contributions, we can make data-driven decisions with greater confidence.\"\n",
    "\n",
    "Slide 3: Case 1 - Success Scenario\n",
    "Title: Success Case Analysis\n",
    "\n",
    "Pitch:\n",
    "\"In this success scenario, the SHAP waterfall graph highlights the key predictors driving the positive outcome. We see that factors such as [Predictor A], [Predictor B], and [Predictor C] have significantly contributed to the success. By understanding these contributions, we can focus on reinforcing these positive influences in our future strategies.\"\n",
    "\n",
    "Slide 4: Case 2 - Failure Scenario\n",
    "Title: Failure Case Analysis\n",
    "\n",
    "Pitch:\n",
    "\"In this failure scenario, the SHAP waterfall graph reveals a critical change in [Predictor X], which led to the negative outcome. This shift had a substantial impact, overshadowing other positive predictors. By recognizing this, we can implement measures to mitigate such risks and improve our decision-making processes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32com.client as win32\n",
    "\n",
    "# Path to the Excel file and existing Word document\n",
    "excel_file = 'path_to_your_excel_file.xlsx'\n",
    "word_file = 'path_to_your_existing_word_file.docx'\n",
    "\n",
    "# Initialize Excel\n",
    "excel = win32.Dispatch('Excel.Application')\n",
    "workbook = excel.Workbooks.Open(excel_file)\n",
    "\n",
    "# Initialize Word and open the existing document\n",
    "word = win32.Dispatch('Word.Application')\n",
    "word.Visible = True\n",
    "doc = word.Documents.Open(word_file)\n",
    "\n",
    "# Initialize Outlook\n",
    "outlook = win32.Dispatch('outlook.application')\n",
    "mail = outlook.CreateItem(0)\n",
    "mail.Subject = 'Excel Sheets Data'\n",
    "mail.To = 'recipient@example.com'\n",
    "\n",
    "# Copy tables from Excel to Word\n",
    "for sheet in workbook.Sheets:\n",
    "    sheet_name = sheet.Name\n",
    "    sheet.Range(sheet.UsedRange.Address).Copy()\n",
    "\n",
    "    # Insert sheet name as a heading in Word\n",
    "    doc.Paragraphs.Add()\n",
    "    doc.Paragraphs.Last.Range.Text = sheet_name\n",
    "    doc.Paragraphs.Last.Range.Font.Bold = True\n",
    "    doc.Paragraphs.Last.Range.Font.Size = 14\n",
    "\n",
    "    # Paste the copied table into Word\n",
    "    doc.Paragraphs.Add()\n",
    "    doc.Paragraphs.Last.Range.PasteExcelTable(False, False, False)\n",
    "    doc.Paragraphs.Add()\n",
    "\n",
    "# Save the Word document\n",
    "doc.Save()\n",
    "\n",
    "# Copy content from Word to Outlook email body\n",
    "doc.Content.Copy()\n",
    "mail.GetInspector.WordEditor.Application.Selection.Paste()\n",
    "\n",
    "# Display the mail\n",
    "mail.Display(True)\n",
    "\n",
    "# Close Excel and Word\n",
    "workbook.Close(SaveChanges=False)\n",
    "excel.Quit()\n",
    "doc.Close(SaveChanges=False)\n",
    "word.Quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32com.client as win32\n",
    "\n",
    "# Path to the Excel file and existing Word document\n",
    "excel_file = 'path_to_your_excel_file.xlsx'\n",
    "word_file = 'path_to_your_existing_word_file.docx'\n",
    "\n",
    "# Initialize Excel\n",
    "excel = win32.Dispatch('Excel.Application')\n",
    "excel.Visible = False  # Set to True if you want to see the Excel application\n",
    "workbook = excel.Workbooks.Open(excel_file)\n",
    "\n",
    "# Initialize Word and open the existing document\n",
    "word = win32.Dispatch('Word.Application')\n",
    "word.Visible = True\n",
    "doc = word.Documents.Open(word_file)\n",
    "\n",
    "# Initialize Outlook\n",
    "outlook = win32.Dispatch('outlook.application')\n",
    "mail = outlook.CreateItem(0)\n",
    "mail.Subject = 'Excel Sheets Data'\n",
    "mail.To = 'recipient@example.com'\n",
    "\n",
    "# Copy tables from Excel to Word\n",
    "for i, sheet in enumerate(workbook.Sheets):\n",
    "    sheet_name = sheet.Name\n",
    "    \n",
    "    if i == 1:  # For the second sheet (index 1), copy the visible cells directly\n",
    "        used_range = sheet.UsedRange.SpecialCells(win32.constants.xlCellTypeVisible)\n",
    "        used_range.Copy()\n",
    "    else:\n",
    "        sheet.Range(sheet.UsedRange.Address).Copy()\n",
    "\n",
    "    # Insert sheet name as a heading in Word\n",
    "    doc.Paragraphs.Add()\n",
    "    doc.Paragraphs.Last.Range.Text = sheet_name\n",
    "    doc.Paragraphs.Last.Range.Font.Bold = True\n",
    "    doc.Paragraphs.Last.Range.Font.Size = 14\n",
    "\n",
    "    # Paste the copied table into Word\n",
    "    doc.Paragraphs.Add()\n",
    "    doc.Paragraphs.Last.Range.PasteExcelTable(False, False, False)\n",
    "    doc.Paragraphs.Add()\n",
    "\n",
    "# Save the Word document\n",
    "doc.Save()\n",
    "\n",
    "# Copy content from Word to Outlook email body\n",
    "doc.Content.Copy()\n",
    "mail.GetInspector.WordEditor.Application.Selection.Paste()\n",
    "\n",
    "# Display the mail\n",
    "mail.Display(True)\n",
    "\n",
    "# Close Excel and Word\n",
    "workbook.Close(SaveChanges=False)\n",
    "excel.Quit()\n",
    "doc.Close(SaveChanges=False)\n",
    "word.Quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32com.client as win32\n",
    "\n",
    "# Path to the Excel file and existing Word document\n",
    "excel_file = 'path_to_your_excel_file.xlsx'\n",
    "word_file = 'path_to_your_existing_word_file.docx'\n",
    "\n",
    "# Initialize Excel\n",
    "excel = win32.Dispatch('Excel.Application')\n",
    "excel.Visible = False  # Set to True if you want to see the Excel application\n",
    "workbook = excel.Workbooks.Open(excel_file)\n",
    "\n",
    "# Initialize Word and open the existing document\n",
    "word = win32.Dispatch('Word.Application')\n",
    "word.Visible = True\n",
    "doc = word.Documents.Open(word_file)\n",
    "\n",
    "# Initialize Outlook\n",
    "outlook = win32.Dispatch('outlook.application')\n",
    "mail = outlook.CreateItem(0)\n",
    "mail.Subject = 'Excel Sheets Data'\n",
    "mail.To = 'recipient@example.com'\n",
    "\n",
    "# Copy tables from Excel to Word\n",
    "for i, sheet in enumerate(workbook.Sheets):\n",
    "    sheet_name = sheet.Name\n",
    "    \n",
    "    if i == 1:  # Apply filter to the second sheet (index 1)\n",
    "        sheet.AutoFilterMode = False  # Clear any existing filters\n",
    "        sheet.Range(\"A1\").AutoFilter(Field=1, Criteria1=\"<>\")  # Filter out blanks in column A\n",
    "        \n",
    "        # Copy the visible cells after filtering\n",
    "        used_range = sheet.UsedRange.SpecialCells(win32.constants.xlCellTypeVisible)\n",
    "        used_range.Copy()\n",
    "    else:\n",
    "        sheet.Range(sheet.UsedRange.Address).Copy()\n",
    "\n",
    "    # Insert sheet name as a heading in Word\n",
    "    doc.Paragraphs.Add()\n",
    "    doc.Paragraphs.Last.Range.Text = sheet_name\n",
    "    doc.Paragraphs.Last.Range.Font.Bold = True\n",
    "    doc.Paragraphs.Last.Range.Font.Size = 14\n",
    "\n",
    "    # Paste the copied table into Word\n",
    "    doc.Paragraphs.Add()\n",
    "    doc.Paragraphs.Last.Range.PasteExcelTable(False, False, False)\n",
    "    doc.Paragraphs.Add()\n",
    "\n",
    "# Save the Word document\n",
    "doc.Save()\n",
    "\n",
    "# Copy content from Word to Outlook email body\n",
    "doc.Content.Copy()\n",
    "mail.GetInspector.WordEditor.Application.Selection.Paste()\n",
    "\n",
    "# Display the mail\n",
    "mail.Display(True)\n",
    "\n",
    "# Close Excel and Word\n",
    "workbook.Close(SaveChanges=False)\n",
    "excel.Quit()\n",
    "doc.Close(SaveChanges=False)\n",
    "word.Quit()\n",
    "\n",
    "# with filter removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert sheet name as a heading in Word\n",
    "    paragraph = doc.Content.Paragraphs.Add()\n",
    "    paragraph.Range.Text = sheet_name\n",
    "    paragraph.Range.Font.Bold = True\n",
    "    paragraph.Range.Font.Size = 14\n",
    "    paragraph.Range.InsertParagraphAfter()\n",
    "\n",
    "    # Paste the copied table into Word\n",
    "    table_paragraph = doc.Content.Paragraphs.Add()\n",
    "    table_paragraph.Range.PasteExcelTable(False, False, False)\n",
    "    table_paragraph.Range.InsertParagraphAfter()\n",
    "\n",
    "# Save and close the Word document\n",
    "doc.Save()\n",
    "doc.Close()\n",
    "\n",
    "# Reopen the Word document to ensure it's not locked\n",
    "doc = word.Documents.Open(word_file)\n",
    "\n",
    "# Copy content from Word to clipboard\n",
    "doc.Content.Copy()\n",
    "\n",
    "# Use clipboard to paste content into Outlook email body\n",
    "mail.GetInspector.Activate()\n",
    "mail.GetInspector.WordEditor.Application.Selection.Paste()\n",
    "\n",
    "# Display the mail\n",
    "mail.Display(True)\n",
    "\n",
    "# Close Excel and Word\n",
    "workbook.Close(SaveChanges=False)\n",
    "excel.Quit()\n",
    "doc.Close(SaveChanges=False)\n",
    "word.Quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_text = f\"Dear recipient,<br><br>Here is the data from Excel sheets.<br><br>You can access the files on the network drive <a href='{network_path}'>here</a>.<br><br>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "starting_text = f\"Dear recipient,<br><br>Here is the data from Excel sheets.<br><br>You can access the files on the network drive <a href='{network_path}'>here</a>.<br><br>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "# URL of the API endpoint\n",
    "url = 'https://api.example.com/endpoint'\n",
    "\n",
    "# Your credentials\n",
    "username = 'your_username'\n",
    "password = 'your_password'\n",
    "\n",
    "# Headers, if you are sending JSON, otherwise use the appropriate Content-Type\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',  # or 'application/x-www-form-urlencoded' for form-data\n",
    "}\n",
    "\n",
    "# Data to be sent in the body\n",
    "# For JSON data\n",
    "data = {\n",
    "    'param1': 'value1',\n",
    "    'param2': 'value2'\n",
    "}\n",
    "\n",
    "# For form-data (use data dictionary directly)\n",
    "# data = {\n",
    "#     'param1': 'value1',\n",
    "#     'param2': 'value2'\n",
    "# }\n",
    "\n",
    "# Make the POST request with basic authentication\n",
    "response = requests.post(url, auth=HTTPBasicAuth(username, password), headers=headers, json=data)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    print('Request was successful')\n",
    "    print(response.json())\n",
    "else:\n",
    "    print('Failed with status code:', response.status_code)\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import shap\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Handle Missing Values\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Encode Categorical Variables and create mapping dictionaries\n",
    "label_encoders = {}\n",
    "label_mappings = {}\n",
    "\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    label_encoders[column] = le\n",
    "    label_mappings[column] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "# Exploratory Data Analysis (EDA)\n",
    "# Correlation Matrix for Numeric Variables\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "# Visualize Relationships\n",
    "sns.pairplot(df, hue='target_variable')\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance with Random Forest\n",
    "# Split Data\n",
    "X = df.drop('target_variable', axis=1)\n",
    "y = df['target_variable']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Feature Importance\n",
    "feature_importances = pd.DataFrame(model.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# SHAP Values for Model Interpretation\n",
    "# Initialize SHAP Explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X)\n",
    "\n",
    "# SHAP Dependence Plot with translated labels for categorical columns\n",
    "for col in X.columns:\n",
    "    if col in label_mappings:\n",
    "        shap.dependence_plot(col, shap_values, X, display_features=X.replace({col: {v: k for k, v in label_mappings[col].items()}}))\n",
    "    else:\n",
    "        shap.dependence_plot(col, shap_values, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have loaded the label_encoders dictionary\n",
    "with open('label_encoders.pkl', 'rb') as f:\n",
    "    label_encoders = pickle.load(f)\n",
    "\n",
    "# Function to get the original label from encoded value\n",
    "def get_original_label(column_name, encoded_value):\n",
    "    if column_name in label_encoders:\n",
    "        encoder = label_encoders[column_name]\n",
    "        original_label = encoder.inverse_transform([encoded_value])\n",
    "        return original_label[0]\n",
    "    else:\n",
    "        return \"Column not found in label encoders\"\n",
    "\n",
    "# Example usage\n",
    "column_name = 'Category'  # Replace with your column name\n",
    "encoded_value = 1         # Replace with the encoded value you want to check\n",
    "original_label = get_original_label(column_name, encoded_value)\n",
    "print(f'The original label for encoded value {encoded_value} in column \"{column_name}\" is: {original_label}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
