{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataframe\n",
    "data = {\n",
    "    'O/S Cash USD': [100, 200, 150, 300, 400],\n",
    "    'Cpty Name': ['Company A', 'Company B', 'Company A', 'Company C', 'Company B'],\n",
    "    'Treated_Comments': ['Comment1', 'Comment2', 'Comment3', 'Comment4', 'Comment5'],\n",
    "    'Treated_Fails_code': ['Code1', 'Code2', 'Code1', 'Code2', 'Code1']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Group by 'Treated_Fails_code' and 'Cpty Name', then count occurrences\n",
    "grouped = df.groupby(['Treated_Fails_code', 'Cpty Name']).size().reset_index(name='Count')\n",
    "\n",
    "# Sort by 'Treated_Fails_code' and 'Count' in descending order\n",
    "sorted_grouped = grouped.sort_values(by=['Treated_Fails_code', 'Count'], ascending=[True, False])\n",
    "\n",
    "# Save the result to an Excel file\n",
    "output_file = 'fails_code_counts.xlsx'\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    sorted_grouped.to_excel(writer, sheet_name='Fails_Code_Counts', index=False)\n",
    "\n",
    "print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample dataframe\n",
    "data = {\n",
    "    'O/S Cash USD': [100, 200, 150, 300, 400],\n",
    "    'Cpty Name': ['Company A', 'Company B', 'Company A', 'Company C', 'Company B'],\n",
    "    'Treated_Comments': [\n",
    "        'Short to Deliver due to pending receipts from CP x',\n",
    "        'Short to Deliver due to failing receipts from Cp b',\n",
    "        'Cp a was short to deliver',\n",
    "        'Firm was Short due to failing receipts from Cp b',\n",
    "        'Cp a was short to deliver'\n",
    "    ],\n",
    "    'Treated_Fails_code': ['FTD', 'FTD', 'FTD', 'FTD', 'Other']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter rows where 'Treated_Fails_code' is 'FTD'\n",
    "ftd_df = df[df['Treated_Fails_code'] == 'FTD'].copy()\n",
    "\n",
    "# Function to extract firm mentioned after 'from' or find other cases of failure to deliver\n",
    "def extract_firm(comment):\n",
    "    # Regular expression to find the firm after 'from'\n",
    "    match = re.search(r'from\\s([\\w\\s]+)', comment, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()  # Return firm name after 'from'\n",
    "    # Handle other patterns like 'Cp a was short to deliver'\n",
    "    match_alt = re.search(r'([\\w\\s]+)\\swas short to deliver', comment, re.IGNORECASE)\n",
    "    if match_alt:\n",
    "        return match_alt.group(1).strip()  # Return firm name before 'was short to deliver'\n",
    "    return None  # If no match is found, return None\n",
    "\n",
    "# Apply the extraction function to the 'Treated_Comments' column\n",
    "ftd_df['Because of Which firm'] = ftd_df['Treated_Comments'].apply(extract_firm)\n",
    "\n",
    "# Count the occurrences of FTD for each 'Cpty Name'\n",
    "ftd_summary = ftd_df.groupby(['Cpty Name', 'Because of Which firm']).size().reset_index(name='Count')\n",
    "\n",
    "# Save the result to the existing Excel file\n",
    "output_file = 'fails_code_counts.xlsx'\n",
    "with pd.ExcelWriter(output_file, mode='a', engine='openpyxl') as writer:\n",
    "    # Save the new analysis in a new sheet\n",
    "    ftd_summary.to_excel(writer, sheet_name='FTD_Analysis', index=False)\n",
    "\n",
    "print(f\"New analysis saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified GPX saved to output.gpx\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def add_name_tags_to_gpx(file_path, output_path):\n",
    "    # Parse the GPX file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Define namespaces used in GPX file\n",
    "    ns = {'gpx': 'http://www.topografix.com/GPX/1/1'}\n",
    "\n",
    "    # Iterate over all waypoints (wpt elements)\n",
    "    for wpt in root.findall('gpx:wpt', ns):\n",
    "        # Check if the waypoint has a 'name' child element\n",
    "        name_tag = wpt.find('gpx:name', ns)\n",
    "        \n",
    "        # If no name tag exists, add an empty <name> tag\n",
    "        if name_tag is None:\n",
    "            ET.SubElement(wpt, '{http://www.topografix.com/GPX/1/1}name').text = ''\n",
    "\n",
    "    # Write the modified tree back to an output file\n",
    "    tree.write(output_path, encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "# Example usage\n",
    "file_path = r'C:\\Users\\shara\\Downloads\\\\28-new-york-city-1-new-york-united-states.gpx'  # Path to the input GPX file\n",
    "output_path = 'output.gpx'  # Path to save the modified GPX file\n",
    "\n",
    "add_name_tags_to_gpx(file_path, output_path)\n",
    "print(f\"Modified GPX saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified GPX saved to 20-kyoto-1-honshu-japan.gpx\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def add_name_tags_to_gpx(file_path, output_path):\n",
    "    # Parse the GPX file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Iterate over all <wpt> elements (waypoints)\n",
    "    for wpt in root.findall('.//wpt'):\n",
    "        # Check if <name> tag exists within the waypoint\n",
    "        name_tag = wpt.find('name')\n",
    "        \n",
    "        # If <name> tag is missing, create an empty <name> tag\n",
    "        if name_tag is None:\n",
    "            name_element = ET.Element('name')\n",
    "            name_element.text = ' '  # Add a space inside the <name> tag\n",
    "            wpt.append(name_element)  # Append the new <name> tag to <wpt>\n",
    "\n",
    "    # Write the modified tree back to an output file\n",
    "    tree.write(output_path, encoding='utf-8', xml_declaration=True)\n",
    "\n",
    "# Example usage\n",
    "fname = '20-kyoto-1-honshu-japan.gpx'\n",
    "file_path = f'C:/Users/shara/Downloads/{fname}'  # Path to the input GPX file\n",
    "output_path = fname  # Path to save the modified GPX file\n",
    "\n",
    "add_name_tags_to_gpx(file_path, output_path)\n",
    "print(f\"Modified GPX saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing AUM (Assets Under Management) vs. Trading Volume is important for evaluating the characteristics, liquidity, and stability of an investment vehicle, especially in the context of exchange-traded funds (ETFs) like IWM (iShares Russell 2000 ETF) or mutual funds. Here's why it's significant to consider both:\n",
    "\n",
    "1. Liquidity\n",
    "AUM reflects the total value of assets held by a fund. Larger AUM generally means the fund has more capital invested in its underlying assets, which can indicate higher liquidity and easier execution of trades without significantly impacting the price.\n",
    "Trading Volume represents how many shares of the fund are traded in the market during a given time period (usually daily volume). High trading volume ensures better liquidity because it means there are more buyers and sellers, allowing investors to enter and exit positions with less price impact.\n",
    "Why it's important: A fund with high AUM but low trading volume might indicate that the fund holds a lot of assets, but there may not be enough daily activity in the market for investors to easily buy or sell shares. On the other hand, high trading volume without significant AUM could signal speculative activity or short-term trading, which might not reflect the long-term strength or stability of the fund.\n",
    "\n",
    "2. Market Impact\n",
    "AUM indicates the scale of a fund’s holdings. Large AUM in an ETF implies that the fund holds a significant portion of the underlying assets. If such a fund starts making large trades (due to inflows or outflows), it could potentially impact the market for those assets.\n",
    "Trading Volume helps investors gauge how much influence market trades have on the fund's price. If trading volume is low, large buy or sell orders can lead to higher volatility and price swings. However, if volume is high, these trades might have little impact.\n",
    "Why it's important: A mismatch between AUM and trading volume can impact how easily an investor can execute large trades. A fund with high AUM and low trading volume could be hard to exit without causing price disruption.\n",
    "\n",
    "3. Tracking Efficiency\n",
    "AUM can influence how well an ETF tracks its benchmark index. Funds with higher AUM typically have better access to resources for managing and balancing the portfolio, which allows them to track their index more accurately.\n",
    "Trading Volume affects the pricing of the ETF relative to its Net Asset Value (NAV). If trading volume is low, the ETF’s market price might deviate from the value of its underlying assets (NAV), leading to pricing inefficiencies.\n",
    "Why it's important: A high-AUM, high-trading-volume ETF tends to offer better tracking of its benchmark index and provide tighter spreads between the ETF price and NAV, making it more efficient for both long-term and short-term investors.\n",
    "\n",
    "4. Cost Efficiency\n",
    "AUM is often linked to the fund's expense ratio. Larger funds can spread operational costs over more assets, which might result in lower expense ratios and make the fund more cost-effective for investors.\n",
    "Trading Volume affects transaction costs for buying and selling shares. Higher volume generally leads to narrower bid-ask spreads, meaning lower costs for traders who want to move in and out of positions.\n",
    "Why it's important: For long-term investors, a fund with high AUM but low trading volume might still be appealing because of its low expense ratio, even if it’s harder to trade. For active traders, high trading volume is more crucial to minimize transaction costs.\n",
    "\n",
    "5. Investor Sentiment and Fund Stability\n",
    "AUM can indicate investor confidence and stability. A growing AUM shows that more investors are pouring money into the fund, which can signal that the fund is stable and in demand.\n",
    "Trading Volume can indicate investor sentiment and market activity. If trading volume spikes, it could indicate increased interest in the fund, either due to market volatility, news, or changes in the underlying assets.\n",
    "Why it's important: A fund with large AUM and consistent trading volume is typically considered more stable and reliable, while a sudden drop in AUM or abnormal trading volume spikes could be red flags for investors.\n",
    "\n",
    "Conclusion:\n",
    "AUM helps measure the size, long-term stability, and efficiency of a fund.\n",
    "Trading Volume helps gauge liquidity, market sentiment, and transaction costs.\n",
    "Analyzing both together provides a comprehensive view of the fund’s liquidity, efficiency, and cost-effectiveness. High AUM with high trading volume generally indicates a well-rounded, liquid, and stable fund, whereas discrepancies between the two can suggest potential liquidity issues or speculative trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Trader Names': ['{Trader A}, {Trader B}', '{Trader C}', '{Trader D}, {Trader E}', '{Trader A}', '{Trader B}'],\n",
    "    'USD Difference': [5000, -7000, 2000, -3000, 1500],\n",
    "    'Fund House': ['Fund X', 'Fund Y', 'Fund Z', 'Fund X', 'Fund Y']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Clean the Trader Names column\n",
    "# Remove curly braces and split multiple traders into lists\n",
    "df['Trader Names'] = df['Trader Names'].str.replace(r'{|}', '', regex=True)\n",
    "df['Trader Names'] = df['Trader Names'].str.split(', ')\n",
    "\n",
    "# Step 2: Expand the DataFrame so each trader has their own row\n",
    "df_expanded = df.explode('Trader Names')\n",
    "\n",
    "# Step 3: Group by Trader Names and sum the USD Difference\n",
    "trader_summary = df_expanded.groupby('Trader Names', as_index=False)['USD Difference'].sum()\n",
    "\n",
    "# Step 4: Sort to identify top positive and negative USD Differences\n",
    "top_negative_traders = trader_summary.sort_values(by='USD Difference').head()\n",
    "top_positive_traders = trader_summary.sort_values(by='USD Difference', ascending=False).head()\n",
    "\n",
    "# Display results\n",
    "print(\"Top Negative Traders:\\n\", top_negative_traders)\n",
    "print(\"Top Positive Traders:\\n\", top_positive_traders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Trader Names': ['Trader A', 'Trader B'],\n",
    "    'USD Difference': [5000, -3000],\n",
    "    'Trade Date': ['31-AUG-24', '15-SEP-24']  # Dates in 'DD-MON-YY' format as strings\n",
    "}\n",
    "\n",
    "# Create DataFrame and convert 'Trade Date' to datetime\n",
    "df = pd.DataFrame(data)\n",
    "df['Trade Date'] = pd.to_datetime(df['Trade Date'], format='%d-%b-%y')\n",
    "\n",
    "# Save to Excel with the date format 'DD-MON-YY'\n",
    "writer = pd.ExcelWriter('trades.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Write the dataframe to Excel\n",
    "df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "# Get the xlsxwriter workbook and worksheet objects\n",
    "workbook  = writer.book\n",
    "worksheet = writer.sheets['Sheet1']\n",
    "\n",
    "# Define a date format\n",
    "date_format = workbook.add_format({'num_format': 'DD-MON-YY'})\n",
    "\n",
    "# Apply the date format to the 'Trade Date' column (assuming it's in the third column, index 2)\n",
    "worksheet.set_column('C:C', None, date_format)\n",
    "\n",
    "# Save the Excel file\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataframe with necessary columns\n",
    "data = {\n",
    "    'OS Cash': [1000, 2000, -1500, 3000, -4000],\n",
    "    'Operation Type': ['BUY', 'SELL', 'REC', 'DEL', 'CREC'],\n",
    "    'WCOFF_RATE': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'FX_RATE': [1.1, 1.2, 1.3, 1.4, 1.5]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the operation types that correspond to the two cases\n",
    "buy_rec_types = ['BUY', 'REC', 'CDEL', 'CSEL']\n",
    "sell_del_types = ['SELL', 'DEL', 'CREC', 'CBUY']\n",
    "\n",
    "# Apply the formula based on the 'Operation Type'\n",
    "df['WCOF USD'] = df.apply(\n",
    "    lambda row: ((-1 * row['OS Cash'] * row['WCOFF_RATE']) / 36000) * row['FX_RATE']\n",
    "    if row['Operation Type'] in buy_rec_types else\n",
    "    ((1 * row['OS Cash'] * row['WCOFF_RATE']) / 36000) * row['FX_RATE'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample dataframe\n",
    "data = {\n",
    "    'O/S Cash USD': [100, -200, 150, -300, 400],\n",
    "    'Cpty Name': ['Company A', 'Company B', 'Company A', 'Company C', 'Company B'],\n",
    "    'Treated_Comments': [\n",
    "        'Short to Deliver due to pending receipts from CP x',\n",
    "        'Short to Deliver due to failing receipts from Cp b',\n",
    "        'Cp a was short to deliver',\n",
    "        'Firm was Short due to failing receipts from Cp b',\n",
    "        'Cp a was short to deliver'\n",
    "    ],\n",
    "    'Treated_Fails_code': ['FTD', 'FTD', 'FTD', 'FTD', 'FTR']  # FTR added for testing positive case\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter rows where 'Treated_Fails_code' is 'FTD' (Fail to Deliver) or 'FTR' (Fail to Receive)\n",
    "ftd_df = df[df['Treated_Fails_code'].isin(['FTD', 'FTR'])].copy()\n",
    "\n",
    "# Function to extract firm mentioned after 'from' or find other cases of failure to deliver (only for FTD rows)\n",
    "def extract_firm(comment, fails_code):\n",
    "    if fails_code == 'FTD':\n",
    "        # Regular expression to find the firm after 'from'\n",
    "        match = re.search(r'from\\s([\\w\\s]+)', comment, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()  # Return firm name after 'from'\n",
    "        # Handle other patterns like 'Cp a was short to deliver'\n",
    "        match_alt = re.search(r'([\\w\\s]+)\\swas short to deliver', comment, re.IGNORECASE)\n",
    "        if match_alt:\n",
    "            return match_alt.group(1).strip()  # Return firm name before 'was short to deliver'\n",
    "    return None  # If no match is found or it's not an FTD row, return None\n",
    "\n",
    "# Apply the extraction function to the 'Treated_Comments' column for only 'FTD' rows\n",
    "ftd_df['Because of Which firm'] = ftd_df.apply(lambda row: extract_firm(row['Treated_Comments'], row['Treated_Fails_code']), axis=1)\n",
    "\n",
    "# Separate columns for positive and negative 'O/S Cash USD'\n",
    "ftd_df['-WCOF'] = ftd_df.apply(lambda x: x['O/S Cash USD'] if x['Treated_Fails_code'] == 'FTD' else None, axis=1)\n",
    "ftd_df['+WCOF'] = ftd_df.apply(lambda x: x['O/S Cash USD'] if x['Treated_Fails_code'] == 'FTR' else None, axis=1)\n",
    "\n",
    "# Count the occurrences of FTD/FTR for each 'Cpty Name' and 'Because of Which firm'\n",
    "ftd_summary = ftd_df.groupby(['Cpty Name', 'Because of Which firm']).agg({\n",
    "    '-WCOF': 'sum',  # Sum of negative WCOF (Fail to Deliver)\n",
    "    '+WCOF': 'sum',  # Sum of positive WCOF (Fail to Receive)\n",
    "    'Treated_Fails_code': 'count'  # Count of occurrences\n",
    "}).reset_index().rename(columns={'Treated_Fails_code': 'Count'})\n",
    "\n",
    "# Save the result to the existing Excel file\n",
    "output_file = 'fails_code_counts_wcof.xlsx'\n",
    "with pd.ExcelWriter(output_file, mode='a', engine='openpyxl') as writer:\n",
    "    # Save the new analysis in a new sheet\n",
    "    ftd_summary.to_excel(writer, sheet_name='FTD_FTR_WCOF_Analysis', index=False)\n",
    "\n",
    "print(f\"New analysis saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1529, 7)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "x = pd.read_csv('C:\\\\Users\\\\shara\\\\Downloads\\\\50100624741139_1729694304537.txt',delimiter=',', on_bad_lines='skip')\n",
    "x.columns = x.columns.str.replace(' ','')\n",
    "y = pd.read_csv('C:\\\\Users\\\\shara\\\\Downloads\\\\50100624741139_1729694704016.txt',delimiter=',', on_bad_lines='skip')\n",
    "y.columns = y.columns.str.replace(' ','')\n",
    "df = pd.concat([x,y], ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Narration</th>\n",
       "      <th>ValueDat</th>\n",
       "      <th>DebitAmount</th>\n",
       "      <th>CreditAmount</th>\n",
       "      <th>Chq/RefNumber</th>\n",
       "      <th>ClosingBalance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25/05/23</td>\n",
       "      <td>NEFT CR-ICIC0002564-SALARY NOMURA MAY 2023-SHA...</td>\n",
       "      <td>25/05/23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141584.0</td>\n",
       "      <td>000CMS3310690898</td>\n",
       "      <td>141584.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25/05/23</td>\n",
       "      <td>UPI-SHARAN DHANPAL SHETT-SHARANSHETTY.001@OKAX...</td>\n",
       "      <td>25/05/23</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0000314595235993</td>\n",
       "      <td>140584.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26/05/23</td>\n",
       "      <td>UPI-SWIGGY-SWIGGYUPI@AXISBANK-UTIB0000000-3146...</td>\n",
       "      <td>26/05/23</td>\n",
       "      <td>242.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0000314641967477</td>\n",
       "      <td>140342.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26/05/23</td>\n",
       "      <td>UPI-SWIGGY-UPISWIGGY@ICICI-ICIC0DC0099-3146426...</td>\n",
       "      <td>26/05/23</td>\n",
       "      <td>237.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0000314642692160</td>\n",
       "      <td>140105.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27/05/23</td>\n",
       "      <td>UPI-QUADRILLION FINANCE -QUADRILLION.SLICE@YES...</td>\n",
       "      <td>27/05/23</td>\n",
       "      <td>3149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0000314757289586</td>\n",
       "      <td>136956.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>22/10/24</td>\n",
       "      <td>UPI-GRAND REGENCY-Q713657152@YBL-YESB0YBLUPI-4...</td>\n",
       "      <td>22/10/24</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0000429619096725</td>\n",
       "      <td>21012.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>22/10/24</td>\n",
       "      <td>UPI-SHIVANANDA S-SHIVANANDA.6919@WAAXIS-BARB0V...</td>\n",
       "      <td>22/10/24</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0000429620010091</td>\n",
       "      <td>20892.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>22/10/24</td>\n",
       "      <td>UPI-VAGESH BUDARAKATTIMA-8095023335-3@IBL-KARB...</td>\n",
       "      <td>22/10/24</td>\n",
       "      <td>899.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0000429644966460</td>\n",
       "      <td>19993.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>22/10/24</td>\n",
       "      <td>UPI-VISALAKSHI MARKETING-PAYPHIVISALAKSHIMKTPV...</td>\n",
       "      <td>22/10/24</td>\n",
       "      <td>340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0000429647554171</td>\n",
       "      <td>19653.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>22/10/24</td>\n",
       "      <td>POS 416021XXXXXX0005 PHICOM*HMS HOST          ...</td>\n",
       "      <td>22/10/24</td>\n",
       "      <td>752.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0000429616246926</td>\n",
       "      <td>18901.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1528 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                                          Narration  \\\n",
       "0      25/05/23    NEFT CR-ICIC0002564-SALARY NOMURA MAY 2023-SHA...   \n",
       "1      25/05/23    UPI-SHARAN DHANPAL SHETT-SHARANSHETTY.001@OKAX...   \n",
       "2      26/05/23    UPI-SWIGGY-SWIGGYUPI@AXISBANK-UTIB0000000-3146...   \n",
       "3      26/05/23    UPI-SWIGGY-UPISWIGGY@ICICI-ICIC0DC0099-3146426...   \n",
       "4      27/05/23    UPI-QUADRILLION FINANCE -QUADRILLION.SLICE@YES...   \n",
       "...           ...                                                ...   \n",
       "1524   22/10/24    UPI-GRAND REGENCY-Q713657152@YBL-YESB0YBLUPI-4...   \n",
       "1525   22/10/24    UPI-SHIVANANDA S-SHIVANANDA.6919@WAAXIS-BARB0V...   \n",
       "1526   22/10/24    UPI-VAGESH BUDARAKATTIMA-8095023335-3@IBL-KARB...   \n",
       "1527   22/10/24    UPI-VISALAKSHI MARKETING-PAYPHIVISALAKSHIMKTPV...   \n",
       "1528   22/10/24    POS 416021XXXXXX0005 PHICOM*HMS HOST          ...   \n",
       "\n",
       "       ValueDat  DebitAmount  CreditAmount            Chq/RefNumber  \\\n",
       "0     25/05/23           0.0      141584.0  000CMS3310690898          \n",
       "1     25/05/23        1000.0           0.0  0000314595235993          \n",
       "2     26/05/23         242.0           0.0  0000314641967477          \n",
       "3     26/05/23         237.0           0.0  0000314642692160          \n",
       "4     27/05/23        3149.0           0.0  0000314757289586          \n",
       "...         ...          ...           ...                      ...   \n",
       "1524  22/10/24         500.0           0.0  0000429619096725          \n",
       "1525  22/10/24         120.0           0.0  0000429620010091          \n",
       "1526  22/10/24         899.0           0.0  0000429644966460          \n",
       "1527  22/10/24         340.0           0.0  0000429647554171          \n",
       "1528  22/10/24         752.5           0.0  0000429616246926          \n",
       "\n",
       "      ClosingBalance  \n",
       "0          141584.00  \n",
       "1          140584.00  \n",
       "2          140342.00  \n",
       "3          140105.00  \n",
       "4          136956.00  \n",
       "...              ...  \n",
       "1524        21012.78  \n",
       "1525        20892.78  \n",
       "1526        19993.78  \n",
       "1527        19653.78  \n",
       "1528        18901.28  \n",
       "\n",
       "[1528 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total spent in Statement: 185162.22000000003\n",
      "Total spent on SWIGGY: 6208.0, Cashback: 620.8000000000001\n",
      "Total spent on Online Shopping: 86858.02, Cashback: 1500\n",
      "Total spent on Other Expenses: 98304.20000000001, Total spent on Other Expenses (excluding exclusions): 47349.2, Cashback: 473.49199999999996\n",
      "Total Savings: 2594.2920000000004\n",
      "Total Savings Yearly: 31131.504000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shara\\AppData\\Local\\Temp\\ipykernel_41288\\2285031644.py:38: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  shopping_spends = df[df['Narration'].str.contains('|'.join(shopping_keywords), case=False, na=False)]\n",
      "C:\\Users\\shara\\AppData\\Local\\Temp\\ipykernel_41288\\2285031644.py:41: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  other_spends = df[~df['Narration'].str.contains('SWIGGY|' + '|'.join(shopping_keywords), case=False, na=False)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = x.copy()\n",
    "# Assuming your bank statement DataFrame is called 'df'\n",
    "# Replace this with the actual DataFrame or CSV file you're working with\n",
    "# df = pd.read_csv('path_to_your_bank_statement.csv')\n",
    "\n",
    "# Filter for spends related to SWIGGY\n",
    "swiggy_spends = df[df['Narration'].str.contains('SWIGGY', case=False, na=False)]\n",
    "\n",
    "# Filter for online shopping spends\n",
    "shopping_keywords = [\n",
    "    \"Myntra\", \"Ajio\", \"Zara\", \"H&M\", \"Fabindia\", \"Biba\", \"Pantaloons\", \n",
    "    \"Shoppers Stop\", \"Lifestyle\", \"Marks & Spencer\", \"Levi's\", \"Max Fashion\", \n",
    "    \"Raymond\", \"Forever 21\", \"AND\", \"Vero Moda\", \"Van Heusen\", \"W\", \n",
    "    \"Peter England\", \"Louis Philippe\", \"Allen Solly\", \"Westside\", \"Reliance Retail\", \n",
    "    \"Big Bazaar\", \"DMart\", \"Shoppers Stop\", \"Lifestyle\", \"Pantaloons\", \n",
    "    \"Max Fashion\", \"Westside\", \"Central\", \"More Retail\", \"Spencer’s Retail\", \n",
    "    \"V-Mart\", \"Croma\", \"Reliance Digital\", \"Vijay Sales\", \"Amazon\", \"Flipkart\", \n",
    "    \"Tata CLiQ\", \"Samsung\", \"Apple\", \"Sony\", \"LG\", \"Dell\", \"HP\", \"Mi (Xiaomi)\", \n",
    "    \"OnePlus\", \"Vivo\", \"Oppo\", \"BookMyShow\", \"PVR Cinemas\", \"INOX\", \n",
    "    \"Sathyam Cinemas\", \"Amazon Prime Video\", \"Netflix\", \"Zee5\", \"Disney+ Hotstar\", \n",
    "    \"Spotify\", \"JioSaavn\", \"Gaana\", \"Tata Sky\", \"Dish TV\", \"Sun Direct\", \n",
    "    \"Pepperfry\", \"Urban Ladder\", \"IKEA\", \"HomeTown\", \"Godrej Interio\", \n",
    "    \"Nilkamal\", \"Durian\", \"Wakefit\", \"Fabindia\", \"Livspace\", \"Asian Paints Home Solutions\", \n",
    "    \"Home Centre\", \"D'Decor\", \"1mg\", \"Netmeds\", \"PharmEasy\", \"Medlife\", \n",
    "    \"Apollo Pharmacy\", \"MedPlus\", \"Wellness Forever\", \"Guardian Pharmacy\", \n",
    "    \"DawaBazaar\", \"Nykaa\", \"Purplle\", \"The Body Shop\", \"Forest Essentials\", \n",
    "    \"Sephora\", \"Lush\", \"Sugar Cosmetics\", \"Mamaearth\", \"WOW Skin Science\", \n",
    "    \"Himalaya\", \"Dabur\", \"Lakme\", \"L'Oréal\", \"Biotique\", \"Khadi Naturals\", \n",
    "    \"Ola\", \"Uber\", \"Zoomcar\", \"Meru Cabs\", \"ANI Technologies\", \"Rapido\", \n",
    "    \"Drivezy\", \"BluSmart\", \"Heads Up For Tails\", \"Pets World\", \"DogSpot\", \n",
    "    \"Petsy\", \"Pawfect\", \"PetKonnect\", \"Just Dogs\", \"Amazon Pets\", \"Flipkart Pets\", \n",
    "    \"PetSutra\", \"Amazon\", \"Flipkart\", \"Meesho\", \"Ajio\", \"Paytm Mall\", \n",
    "    \"Snapdeal\", \"ShopClues\", \"Club Factory\"\n",
    "]\n",
    "\n",
    "\n",
    "shopping_spends = df[df['Narration'].str.contains('|'.join(shopping_keywords), case=False, na=False)]\n",
    "\n",
    "# Filter for other spends (not related to SWIGGY or online shopping)\n",
    "other_spends = df[~df['Narration'].str.contains('SWIGGY|' + '|'.join(shopping_keywords), case=False, na=False)]\n",
    "\n",
    "# Sum the Debit Amount for each category\n",
    "swiggy_total = swiggy_spends['DebitAmount'].sum()\n",
    "shopping_total = shopping_spends['DebitAmount'].sum()\n",
    "other_total = other_spends['DebitAmount'].sum()\n",
    "total = df['DebitAmount'].sum()\n",
    "# # Print the results\n",
    "# print(f\"Total spent in Statement: {total}\")\n",
    "# print(f\"Total spent on SWIGGY: {swiggy_total}\")\n",
    "# print(f\"Total spent on Online Shopping: {shopping_total}\")\n",
    "# print(f\"Total spent on Other Expenses: {other_total}\")\n",
    "\n",
    "# List of keywords to exclude from 'Other' category\n",
    "exclusion_keywords = ['Fuel', 'Rent', 'EMI', 'Wallet', 'Jewellery', 'Government']\n",
    "\n",
    "# Filter other spends excluding the categories mentioned\n",
    "other_spends_excluded = other_spends[~other_spends['Narration'].str.contains('|'.join(exclusion_keywords), case=False, na=False)]\n",
    "\n",
    "# Calculate cashback for SWIGGY, Shopping, and Other spends\n",
    "swiggy_cashback = min(swiggy_total * 0.10, 1500)  # 10% cashback capped at 1500\n",
    "shopping_cashback = min(shopping_total * 0.05, 1500)  # 5% cashback capped at 1500\n",
    "other_spends_total = other_spends_excluded['DebitAmount'].sum()\n",
    "other_cashback = min(other_spends_total * 0.01, 500)  # 1% cashback capped at 500\n",
    "\n",
    "# Calculate total savings\n",
    "total_savings = swiggy_cashback + shopping_cashback + other_cashback\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total spent in Statement: {total}\")\n",
    "print(f\"Total spent on SWIGGY: {swiggy_total}, Cashback: {swiggy_cashback}\")\n",
    "print(f\"Total spent on Online Shopping: {shopping_total}, Cashback: {shopping_cashback}\")\n",
    "print(f\"Total spent on Other Expenses: {other_spends['DebitAmount'].sum()}, Total spent on Other Expenses (excluding exclusions): {other_spends_total}, Cashback: {other_cashback}\")\n",
    "print(f\"Total Savings: {total_savings}\")\n",
    "print(f\"Total Savings Yearly: {total_savings*12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shara\\AppData\\Local\\Temp\\ipykernel_6364\\2292688736.py:55: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  shopping_spends = monthly_data[monthly_data['Narration'].str.contains('|'.join(shopping_keywords), case=False, na=False)]\n",
      "C:\\Users\\shara\\AppData\\Local\\Temp\\ipykernel_6364\\2292688736.py:58: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  other_spends = monthly_data[~monthly_data['Narration'].str.contains('SWIGGY|' + '|'.join(shopping_keywords), case=False, na=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Month  Swiggy Spend  Swiggy Cashback  Shopping Spend  Shopping Cashback  \\\n",
      "0   2023-05        3411.0            341.1        29455.42          1472.7710   \n",
      "1   2023-06        1351.0            135.1        22890.00          1144.5000   \n",
      "2   2023-07        1337.0            133.7        20509.84          1025.4920   \n",
      "3   2023-08         227.0             22.7         6812.00           340.6000   \n",
      "4   2023-09        3217.0            321.7         7169.23           358.4615   \n",
      "5   2023-10        3483.0            348.3        16752.58           837.6290   \n",
      "6   2023-11        5490.0            549.0       110109.78          1500.0000   \n",
      "7   2023-12        5257.0            525.7        53722.11          1500.0000   \n",
      "8   2024-01        5359.0            535.9        21355.45          1067.7725   \n",
      "9   2024-02        8146.0            814.6        37705.34          1500.0000   \n",
      "10  2024-03        7824.0            782.4        37549.21          1500.0000   \n",
      "11  2024-04        3002.0            300.2        93587.15          1500.0000   \n",
      "12  2024-05        8215.0            821.5       730831.30          1500.0000   \n",
      "13  2024-06        2337.0            233.7        25649.42          1282.4710   \n",
      "14  2024-07        6316.0            631.6        77673.80          1500.0000   \n",
      "15  2024-08        3581.0            358.1        98418.98          1500.0000   \n",
      "16  2024-09        3775.0            377.5        73930.44          1500.0000   \n",
      "17  2024-10        5053.0            505.3        26677.39          1333.8695   \n",
      "\n",
      "    Other Spend (Excluding exclusions)  Other Cashback  Total Cashback  \n",
      "0                             30553.70        305.5370       2119.4080  \n",
      "1                             50283.23        500.0000       1779.6000  \n",
      "2                             66240.77        500.0000       1659.1920  \n",
      "3                            157900.67        500.0000        863.3000  \n",
      "4                             58675.33        500.0000       1180.1615  \n",
      "5                             50997.08        500.0000       1685.9290  \n",
      "6                            160302.80        500.0000       2549.0000  \n",
      "7                            101923.54        500.0000       2525.7000  \n",
      "8                             85039.25        500.0000       2103.6725  \n",
      "9                             39582.68        395.8268       2710.4268  \n",
      "10                            56639.54        500.0000       2782.4000  \n",
      "11                            86559.00        500.0000       2300.2000  \n",
      "12                           210297.40        500.0000       2821.5000  \n",
      "13                           104091.82        500.0000       2016.1710  \n",
      "14                           111819.40        500.0000       2631.6000  \n",
      "15                            40875.24        408.7524       2266.8524  \n",
      "16                            39932.62        399.3262       2276.8262  \n",
      "17                            39957.30        399.5730       2238.7425  \n",
      "Total Savings Yearly: 38510.6819\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'Date' column contains the date of the transaction and is in string format\n",
    "# Convert the 'Date' column to datetime format if not already done\n",
    "df.dropna(subset=['Date'],inplace=True)\n",
    "df['Date'] = df['Date'].str.replace(' ','')\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\n",
    "# Add a new column for year and month\n",
    "df['YearMonth'] = df['Date'].dt.to_period('M')  # 'M' groups by month-year\n",
    "\n",
    "# List of keywords to exclude from 'Other' category\n",
    "exclusion_keywords = ['Fuel', 'Rent', 'EMI', 'Wallet', 'Jewellery', 'Government']\n",
    "\n",
    "# Create a list to store the results for each month\n",
    "monthly_report = []\n",
    "\n",
    "# Filter for online shopping spends\n",
    "shopping_keywords = [\n",
    "    \"Myntra\", \"Ajio\", \"Zara\", \"H&M\", \"Fabindia\", \"Biba\", \"Pantaloons\", \n",
    "    \"Shoppers Stop\", \"Lifestyle\", \"Marks & Spencer\", \"Levi's\", \"Max Fashion\", \n",
    "    \"Raymond\", \"Forever 21\", \"AND\", \"Vero Moda\", \"Van Heusen\", \"W\", \n",
    "    \"Peter England\", \"Louis Philippe\", \"Allen Solly\", \"Westside\", \"Reliance Retail\", \n",
    "    \"Big Bazaar\", \"DMart\", \"Shoppers Stop\", \"Lifestyle\", \"Pantaloons\", \n",
    "    \"Max Fashion\", \"Westside\", \"Central\", \"More Retail\", \"Spencer’s Retail\", \n",
    "    \"V-Mart\", \"Croma\", \"Reliance Digital\", \"Vijay Sales\", \"Amazon\", \"Flipkart\", \n",
    "    \"Tata CLiQ\", \"Samsung\", \"Apple\", \"Sony\", \"LG\", \"Dell\", \"HP\", \"Mi (Xiaomi)\", \n",
    "    \"OnePlus\", \"Vivo\", \"Oppo\", \"BookMyShow\", \"PVR Cinemas\", \"INOX\", \n",
    "    \"Sathyam Cinemas\", \"Amazon Prime Video\", \"Netflix\", \"Zee5\", \"Disney+ Hotstar\", \n",
    "    \"Spotify\", \"JioSaavn\", \"Gaana\", \"Tata Sky\", \"Dish TV\", \"Sun Direct\", \n",
    "    \"Pepperfry\", \"Urban Ladder\", \"IKEA\", \"HomeTown\", \"Godrej Interio\", \n",
    "    \"Nilkamal\", \"Durian\", \"Wakefit\", \"Fabindia\", \"Livspace\", \"Asian Paints Home Solutions\", \n",
    "    \"Home Centre\", \"D'Decor\", \"1mg\", \"Netmeds\", \"PharmEasy\", \"Medlife\", \n",
    "    \"Apollo Pharmacy\", \"MedPlus\", \"Wellness Forever\", \"Guardian Pharmacy\", \n",
    "    \"DawaBazaar\", \"Nykaa\", \"Purplle\", \"The Body Shop\", \"Forest Essentials\", \n",
    "    \"Sephora\", \"Lush\", \"Sugar Cosmetics\", \"Mamaearth\", \"WOW Skin Science\", \n",
    "    \"Himalaya\", \"Dabur\", \"Lakme\", \"L'Oréal\", \"Biotique\", \"Khadi Naturals\", \n",
    "    \"Ola\", \"Uber\", \"Zoomcar\", \"Meru Cabs\", \"ANI Technologies\", \"Rapido\", \n",
    "    \"Drivezy\", \"BluSmart\", \"Heads Up For Tails\", \"Pets World\", \"DogSpot\", \n",
    "    \"Petsy\", \"Pawfect\", \"PetKonnect\", \"Just Dogs\", \"Amazon Pets\", \"Flipkart Pets\", \n",
    "    \"PetSutra\", \"Amazon\", \"Flipkart\", \"Meesho\", \"Ajio\", \"Paytm Mall\", \n",
    "    \"Snapdeal\", \"ShopClues\", \"Club Factory\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each unique month\n",
    "for month in df['YearMonth'].unique():\n",
    "    # Filter the data for the current month\n",
    "    monthly_data = df[df['YearMonth'] == month].copy()\n",
    "    \n",
    "    # Filter for spends related to SWIGGY\n",
    "    swiggy_spends = monthly_data[monthly_data['Narration'].str.contains('SWIGGY', case=False, na=False)]\n",
    "    \n",
    "    # Filter for online shopping spends\n",
    "    shopping_spends = monthly_data[monthly_data['Narration'].str.contains('|'.join(shopping_keywords), case=False, na=False)]\n",
    "    \n",
    "    # Filter for other spends (not related to SWIGGY or online shopping)\n",
    "    other_spends = monthly_data[~monthly_data['Narration'].str.contains('SWIGGY|' + '|'.join(shopping_keywords), case=False, na=False)]\n",
    "    \n",
    "    # Exclude spends related to Fuel, Rent, EMI, Wallet, Jewellery, and Government\n",
    "    other_spends_excluded = other_spends[~other_spends['Narration'].str.contains('|'.join(exclusion_keywords), case=False, na=False)]\n",
    "    \n",
    "    # Sum the Debit Amount for each category\n",
    "    swiggy_total = swiggy_spends['DebitAmount'].sum()\n",
    "    shopping_total = shopping_spends['DebitAmount'].sum()\n",
    "    other_total = other_spends_excluded['DebitAmount'].sum()\n",
    "    \n",
    "    # Calculate cashback for each category\n",
    "    swiggy_cashback = min(swiggy_total * 0.10, 1500)  # 10% cashback capped at 1500\n",
    "    shopping_cashback = min(shopping_total * 0.05, 1500)  # 5% cashback capped at 1500\n",
    "    other_cashback = min(other_total * 0.01, 500)  # 1% cashback capped at 500\n",
    "    \n",
    "    # Calculate total cashback for the month\n",
    "    total_savings = swiggy_cashback + shopping_cashback + other_cashback\n",
    "    \n",
    "    # Store the results for the month\n",
    "    monthly_report.append({\n",
    "        'Month': str(month),\n",
    "        'Swiggy Spend': swiggy_total,\n",
    "        'Swiggy Cashback': swiggy_cashback,\n",
    "        'Shopping Spend': shopping_total,\n",
    "        'Shopping Cashback': shopping_cashback,\n",
    "        'Other Spend (Excluding exclusions)': other_total,\n",
    "        'Other Cashback': other_cashback,\n",
    "        'Total Cashback': total_savings\n",
    "    })\n",
    "\n",
    "# Convert the report into a DataFrame\n",
    "monthly_report_df = pd.DataFrame(monthly_report)\n",
    "\n",
    "# Display the monthly report\n",
    "print(monthly_report_df)\n",
    "\n",
    "# If you want to save this report to a CSV file\n",
    "monthly_report_df.to_csv('C:\\\\Users\\\\shara\\\\Downloads\\\\monthly_cashback_report.csv', index=False)\n",
    "\n",
    "# Optionally, you can calculate yearly savings by summing the total cashback\n",
    "yearly_savings = monthly_report_df['Total Cashback'].sum()\n",
    "print(f\"Total Savings Yearly: {yearly_savings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed: 00b11285-b263-4977-9393-e2fe89d7fd28.JPG -> 00b11285-b263-4977-9393-e2fe89d7fd28.jpg\n",
      "Renamed: 00d8fea2-540b-4daa-ad5e-7ffcd36217b8.JPG -> 00d8fea2-540b-4daa-ad5e-7ffcd36217b8.jpg\n",
      "Renamed: 0a02a8c7-5e7e-4b63-8f2e-10af15c89270 2.JPG -> 0a02a8c7-5e7e-4b63-8f2e-10af15c89270 2.jpg\n",
      "Renamed: 0a02a8c7-5e7e-4b63-8f2e-10af15c89270.JPG -> 0a02a8c7-5e7e-4b63-8f2e-10af15c89270.jpg\n",
      "Renamed: 0a1b25a0-4db8-4d8f-9fa6-b7d773cd0fa4.JPG -> 0a1b25a0-4db8-4d8f-9fa6-b7d773cd0fa4.jpg\n",
      "Renamed: 0ad45118-586e-4dd0-a726-8ad3685f46e8.JPG -> 0ad45118-586e-4dd0-a726-8ad3685f46e8.jpg\n",
      "Renamed: 0b7e6db1-af1f-4137-807c-34139a79d82d.JPG -> 0b7e6db1-af1f-4137-807c-34139a79d82d.jpg\n",
      "Renamed: 0b945af0-bcd0-4126-bd13-108b1e9b888a.JPG -> 0b945af0-bcd0-4126-bd13-108b1e9b888a.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def convert_jpg_to_lowercase(directory):\n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the file has a .JPG extension\n",
    "        if filename.endswith('.JPG'):\n",
    "            # Define old and new file paths\n",
    "            old_file = os.path.join(directory, filename)\n",
    "            new_file = os.path.join(directory, filename[:-4] + '.jpg')\n",
    "            \n",
    "            # Rename the file to have a lowercase .jpg extension\n",
    "            os.rename(old_file, new_file)\n",
    "            print(f\"Renamed: {filename} -> {filename[:-4] + '.jpg'}\")\n",
    "\n",
    "# Specify the directory containing the .JPG files\n",
    "directory_path = 'D:\\Sharan\\\\backup gallery test'\n",
    "convert_jpg_to_lowercase(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00d8fea2-540b-4daa-ad5e-7ffcd36217b8.jpg is within limit: 0.76 MP\n",
      "0a02a8c7-5e7e-4b63-8f2e-10af15c89270 2.jpg is within limit: 1.23 MP\n",
      "0a02a8c7-5e7e-4b63-8f2e-10af15c89270.jpg is within limit: 1.23 MP\n",
      "0a1b25a0-4db8-4d8f-9fa6-b7d773cd0fa4.jpg is within limit: 1.44 MP\n",
      "0ad45118-586e-4dd0-a726-8ad3685f46e8.jpg is within limit: 1.94 MP\n",
      "0b7e6db1-af1f-4137-807c-34139a79d82d.jpg is within limit: 1.23 MP\n",
      "0b945af0-bcd0-4126-bd13-108b1e9b888a.jpg is within limit: 1.23 MP\n",
      "kiryu.jpg is within limit: 2.36 MP\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def check_megapixels(directory, max_megapixels=200):\n",
    "    for filename in os.listdir(directory):\n",
    "        # Process only .jpg or .JPG files\n",
    "        if filename.lower().endswith('.jpg'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    width, height = img.size\n",
    "                    megapixels = (width * height) / 1_000_000  # Calculate in megapixels\n",
    "                    \n",
    "                    if megapixels > max_megapixels:\n",
    "                        print(f\"{filename} exceeds {max_megapixels} MP: {megapixels:.2f} MP\")\n",
    "                    else:\n",
    "                        print(f\"{filename} is within limit: {megapixels:.2f} MP\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Specify the directory containing the images\n",
    "directory_path = 'D:\\Sharan\\\\backup gallery test'\n",
    "check_megapixels(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted: 00b11285-b263-4977-9393-e2fe89d7fd28.JPG -> D:/Sharan/backup gallery test\\00b11285-b263-4977-9393-e2fe89d7fd28.jpg\n",
      "Converted: 00d8fea2-540b-4daa-ad5e-7ffcd36217b8.JPG -> D:/Sharan/backup gallery test\\00d8fea2-540b-4daa-ad5e-7ffcd36217b8.jpg\n",
      "Converted: 0a02a8c7-5e7e-4b63-8f2e-10af15c89270 2.JPG -> D:/Sharan/backup gallery test\\0a02a8c7-5e7e-4b63-8f2e-10af15c89270 2.jpg\n",
      "Converted: 0a02a8c7-5e7e-4b63-8f2e-10af15c89270.JPG -> D:/Sharan/backup gallery test\\0a02a8c7-5e7e-4b63-8f2e-10af15c89270.jpg\n",
      "Converted: 0a1b25a0-4db8-4d8f-9fa6-b7d773cd0fa4.JPG -> D:/Sharan/backup gallery test\\0a1b25a0-4db8-4d8f-9fa6-b7d773cd0fa4.jpg\n",
      "Converted: 0ad45118-586e-4dd0-a726-8ad3685f46e8.JPG -> D:/Sharan/backup gallery test\\0ad45118-586e-4dd0-a726-8ad3685f46e8.jpg\n",
      "Converted: 0b7e6db1-af1f-4137-807c-34139a79d82d.JPG -> D:/Sharan/backup gallery test\\0b7e6db1-af1f-4137-807c-34139a79d82d.jpg\n",
      "Converted: 0b945af0-bcd0-4126-bd13-108b1e9b888a.JPG -> D:/Sharan/backup gallery test\\0b945af0-bcd0-4126-bd13-108b1e9b888a.jpg\n",
      "Converted: 0c48b45e-7a49-4bc9-8009-fe80921c4b34.JPG -> D:/Sharan/backup gallery test\\0c48b45e-7a49-4bc9-8009-fe80921c4b34.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import piexif\n",
    "\n",
    "def convert_to_jpg(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff')):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    # Load EXIF data\n",
    "                    exif_data = piexif.load(img.info.get(\"exif\", b\"\"))\n",
    "                    date_taken = exif_data.get('Exif', {}).get(piexif.ExifIFD.DateTimeOriginal)\n",
    "                    \n",
    "                    # Convert and save as JPG while preserving EXIF data\n",
    "                    new_file = file_path.rsplit('.', 1)[0] + \".jpg\"\n",
    "                    img.convert(\"RGB\").save(new_file, \"JPEG\", exif=piexif.dump(exif_data))\n",
    "                    print(f\"Converted: {filename} -> {new_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not convert {filename}: {e}\")\n",
    "\n",
    "def resize_large_images(directory, max_pixels=200):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith('.jpg'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    # Load EXIF data\n",
    "                    exif_data = piexif.load(img.info.get(\"exif\", b\"\"))\n",
    "                    \n",
    "                    # Check image size in megapixels\n",
    "                    width, height = img.size\n",
    "                    megapixels = (width * height) / 1_000_000\n",
    "                    \n",
    "                    if megapixels > max_pixels:\n",
    "                        scale_factor = (max_pixels * 1_000_000 / (width * height)) ** 0.5\n",
    "                        new_size = (int(width * scale_factor), int(height * scale_factor))\n",
    "                        img = img.resize(new_size, Image.ANTIALIAS)\n",
    "                        \n",
    "                        # Save resized image with original EXIF data\n",
    "                        img.save(file_path, \"JPEG\", exif=piexif.dump(exif_data))\n",
    "                        print(f\"Resized {filename} to {new_size}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not resize {filename}: {e}\")\n",
    "\n",
    "# Use the directory path containing your photos\n",
    "directory_path = 'D:/Sharan/backup gallery test'\n",
    "convert_to_jpg(directory_path)\n",
    "resize_large_images(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import win32file\n",
    "import win32con\n",
    "import pywintypes\n",
    "\n",
    "def set_created_time_to_modified(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith('.mp4'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            \n",
    "            if os.path.isfile(file_path):\n",
    "                # Get the modified time and convert to pywintypes.Time object\n",
    "                modified_time = os.path.getmtime(file_path)\n",
    "                modified_time_as_filetime = pywintypes.Time(modified_time)\n",
    "\n",
    "                # Set creation time to match modified time\n",
    "                handle = win32file.CreateFile(\n",
    "                    file_path,\n",
    "                    win32con.GENERIC_WRITE,\n",
    "                    win32con.FILE_SHARE_WRITE,\n",
    "                    None,\n",
    "                    win32con.OPEN_EXISTING,\n",
    "                    win32con.FILE_ATTRIBUTE_NORMAL,\n",
    "                    None\n",
    "                )\n",
    "                try:\n",
    "                    win32file.SetFileTime(handle, modified_time_as_filetime, None, None)\n",
    "                finally:\n",
    "                    handle.close()  # Close the handle after setting the time\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = 'D:/Sharan/Backup Gallery'\n",
    "set_created_time_to_modified(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define paths to Folder A and Folder B\n",
    "main_folder = \"D:\\Sharan\\Backup Gallery\"\n",
    "unbackup_folder = \"D:\\Sharan\\Backup G\"\n",
    "\n",
    "# Get sets of file names in each folder\n",
    "files_a = set(os.listdir(main_folder))\n",
    "files_b = set(os.listdir(unbackup_folder))\n",
    "\n",
    "# Find the intersection of files between Folder A and Folder B\n",
    "common_files = files_a.intersection(files_b)\n",
    "\n",
    "# Delete files in Folder A that are not present in Folder B\n",
    "for file_name in files_a:\n",
    "    if file_name not in common_files:\n",
    "        os.remove(os.path.join(main_folder, file_name))\n",
    "        # print(f\"Deleted {file_name} from Folder A\")\n",
    "\n",
    "# Delete all files in Folder B\n",
    "for file_name in files_b:\n",
    "    if file_name in common_files:\n",
    "        os.remove(os.path.join(unbackup_folder, file_name))\n",
    "        # print(f\"Deleted {file_name} from Folder B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting images to JPG: 100%|██████████| 13/13 [00:00<00:00, 154.78it/s]\n",
      "Resizing large images: 100%|██████████| 13/13 [00:00<00:00, 928.26it/s]\n",
      "Converting videos: 100%|██████████| 10/10 [00:59<00:00,  5.90s/it]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import piexif\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_to_jpg(directory):\n",
    "    for filename in tqdm(os.listdir(directory), desc=\"Converting images to JPG\"):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff')):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    # Load EXIF data\n",
    "                    exif_data = piexif.load(img.info.get(\"exif\", b\"\"))\n",
    "                    \n",
    "                    # Convert and save as JPG while preserving EXIF data\n",
    "                    new_file = file_path.rsplit('.', 1)[0] + \".jpg\"\n",
    "                    img.convert(\"RGB\").save(new_file, \"JPEG\", exif=piexif.dump(exif_data))\n",
    "            except Exception as e:\n",
    "                pass  # Ignore errors without printing anything\n",
    "\n",
    "def resize_large_images(directory, max_pixels=200):\n",
    "    for filename in tqdm(os.listdir(directory), desc=\"Resizing large images\"):\n",
    "        if filename.lower().endswith('.jpg'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    # Load EXIF data\n",
    "                    exif_data = piexif.load(img.info.get(\"exif\", b\"\"))\n",
    "                    \n",
    "                    # Check image size in megapixels\n",
    "                    width, height = img.size\n",
    "                    megapixels = (width * height) / 1_000_000\n",
    "                    \n",
    "                    if megapixels > max_pixels:\n",
    "                        scale_factor = (max_pixels * 1_000_000 / (width * height)) ** 0.5\n",
    "                        new_size = (int(width * scale_factor), int(height * scale_factor))\n",
    "                        img = img.resize(new_size, Image.ANTIALIAS)\n",
    "                        \n",
    "                        # Save resized image with original EXIF data\n",
    "                        img.save(file_path, \"JPEG\", exif=piexif.dump(exif_data))\n",
    "            except Exception as e:\n",
    "                pass  # Ignore errors without printing anything\n",
    "\n",
    "def convert_videos_to_google_photos_compatible(directory):\n",
    "    converted_videos_folder = os.path.join(directory, \"converted_videos\")\n",
    "    os.makedirs(converted_videos_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "    # Get list of all MP4 and MOV video files in the directory\n",
    "    video_files = [f for f in os.listdir(directory) if f.lower().endswith(('.mp4', '.mov'))]\n",
    "\n",
    "    # Use tqdm for progress tracking with the count of video files\n",
    "    for filename in tqdm(video_files, desc=\"Converting videos\"):\n",
    "        input_file = os.path.join(directory, filename)\n",
    "        output_file = os.path.join(converted_videos_folder, filename.rsplit('.', 1)[0] + \".mp4\")\n",
    "        \n",
    "        # Run FFmpeg command\n",
    "        try:\n",
    "            subprocess.run([\n",
    "                'ffmpeg', '-i', input_file,\n",
    "                '-c:v', 'libx264', '-c:a', 'aac', '-strict', 'experimental', '-b:a', '192k', output_file\n",
    "            ], check=True)\n",
    "        except Exception as e:\n",
    "            pass  # Ignore errors without printing anything\n",
    "\n",
    "        # Delete the original video file\n",
    "        os.remove(input_file)\n",
    "\n",
    "# Use the directory path containing your photos and videos\n",
    "directory_path = 'D:/Sharan/Backup G'\n",
    "convert_to_jpg(directory_path)\n",
    "resize_large_images(directory_path)\n",
    "convert_videos_to_google_photos_compatible(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting images to JPG: 100%|██████████| 155/155 [00:00<00:00, 376.03it/s]\n",
      "Resizing large images: 100%|██████████| 155/155 [00:00<00:00, 3148.21it/s]\n",
      "Converting videos: 100%|██████████| 131/131 [12:37<00:00,  5.78s/it]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import piexif\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "def convert_to_jpg(directory):\n",
    "    for filename in tqdm(os.listdir(directory), desc=\"Converting images to JPG\"):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.heic')):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    # Load EXIF data\n",
    "                    exif_data = piexif.load(img.info.get(\"exif\", b\"\"))\n",
    "                    \n",
    "                    # Convert and save as JPG while preserving EXIF data\n",
    "                    new_file = file_path.rsplit('.', 1)[0] + \".jpg\"\n",
    "                    img.convert(\"RGB\").save(new_file, \"JPEG\", exif=piexif.dump(exif_data))\n",
    "            except Exception:\n",
    "                pass  # Ignore errors without printing anything\n",
    "\n",
    "def resize_large_images(directory, max_pixels=200):\n",
    "    for filename in tqdm(os.listdir(directory), desc=\"Resizing large images\"):\n",
    "        if filename.lower().endswith('.jpg'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    # Load EXIF data\n",
    "                    exif_data = piexif.load(img.info.get(\"exif\", b\"\"))\n",
    "                    \n",
    "                    # Check image size in megapixels\n",
    "                    width, height = img.size\n",
    "                    megapixels = (width * height) / 1_000_000\n",
    "                    \n",
    "                    if megapixels > max_pixels:\n",
    "                        scale_factor = (max_pixels * 1_000_000 / (width * height)) ** 0.5\n",
    "                        new_size = (int(width * scale_factor), int(height * scale_factor))\n",
    "                        img = img.resize(new_size, Image.ANTIALIAS)\n",
    "                        \n",
    "                        # Save resized image with original EXIF data\n",
    "                        img.save(file_path, \"JPEG\", exif=piexif.dump(exif_data))\n",
    "            except Exception:\n",
    "                pass  # Ignore errors without printing anything\n",
    "\n",
    "def convert_videos_to_google_photos_compatible(directory):\n",
    "    converted_videos_folder = os.path.join(directory, \"converted_videos\")\n",
    "    os.makedirs(converted_videos_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "    # Get list of all MP4 and MOV video files in the directory\n",
    "    video_files = [f for f in os.listdir(directory) if f.lower().endswith(('.mp4', '.mov'))]\n",
    "\n",
    "    # Use tqdm for progress tracking with the count of video files\n",
    "    for filename in tqdm(video_files, desc=\"Converting videos\"):\n",
    "        input_file = os.path.join(directory, filename)\n",
    "        output_file = os.path.join(converted_videos_folder, filename.rsplit('.', 1)[0] + \".mp4\")\n",
    "        \n",
    "        # Get original creation and modification times\n",
    "        original_atime = os.path.getatime(input_file)\n",
    "        original_mtime = os.path.getmtime(input_file)\n",
    "        \n",
    "        # Run FFmpeg command\n",
    "        try:\n",
    "            subprocess.run([\n",
    "                'ffmpeg', '-i', input_file,\n",
    "                '-c:v', 'libx264', '-c:a', 'aac', '-strict', 'experimental', '-b:a', '192k', output_file\n",
    "            ], check=True)\n",
    "        except Exception:\n",
    "            pass  # Ignore errors without printing anything\n",
    "\n",
    "        # Set the creation and modification times to match the original\n",
    "        os.utime(output_file, (original_atime, original_mtime))\n",
    "\n",
    "        # Delete the original video file\n",
    "        os.remove(input_file)\n",
    "\n",
    "# Use the directory path containing your photos and videos\n",
    "directory_path = 'D:/Sharan/Backup Gallery'\n",
    "convert_to_jpg(directory_path)\n",
    "resize_large_images(directory_path)\n",
    "convert_videos_to_google_photos_compatible(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def resize_images(directory, target_size=(1440, 2960)):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith('.png'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            \n",
    "            # Open, resize, and save the image\n",
    "            with Image.open(file_path) as img:\n",
    "                resized_img = img.resize(target_size, Image.LANCZOS)\n",
    "                resized_img.save(file_path)  # Overwrite the original file with the resized image\n",
    "\n",
    "# Specify the directory containing your PNG images\n",
    "directory_path = 'D:\\Sharan\\s8 root\\Boot Anims\\DokkOS\\part0'\n",
    "resize_images(directory_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
